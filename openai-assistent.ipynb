{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name='Math tutor',\n",
    "    instructions='You are a personal math tutor. Write and run code to answer math questions.',\n",
    "    tools=[{'type': 'code_interpreter'}],\n",
    "    model='gpt-3.5-turbo',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_QyeByGlS8DkctSed6RlUKGFG', created_at=1720044423, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create()\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = 'user',\n",
    "    content = 'Solve this problem: 3x + 11 = 14'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_Pyot3szBNKnasUrun5zSPk3d', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Solve this problem: 3x + 11 = 14'), type='text')], created_at=1720044424, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_QyeByGlS8DkctSed6RlUKGFG')\n"
     ]
    }
   ],
   "source": [
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id = thread.id,\n",
    "    \n",
    "    assistant_id= assistant.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.retrieve(\n",
    "    thread_id = thread.id,\n",
    "    run_id = run.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[Message](data=[Message(id='msg_o8yXzmcGVloLqV9G1RoMSYhP', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='How does memgpt allow LLMs to have unlimited context length?'), type='text')], created_at=1720044430, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_qzRh9tww3y5X0WVK2SKtfp3D'), Message(id='msg_4F39lKrtExI1J905rXw8WQkX', assistant_id=None, attachments=[Attachment(file_id='file-RvM6HYNffOL32MiXJn1T9YQM', tools=[AttachmentToolAssistantToolsFileSearchTypeOnly(type='file_search')])], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='How does memgpt allow LLMs to have unlimited context length?'), type='text')], created_at=1720044430, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_qzRh9tww3y5X0WVK2SKtfp3D')], object='list', first_id='msg_o8yXzmcGVloLqV9G1RoMSYhP', last_id='msg_4F39lKrtExI1J905rXw8WQkX', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id = thread.id\n",
    ")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Solve this problem: 3x + 11 = 14\n"
     ]
    }
   ],
   "source": [
    "for message in reversed(messages.data):\n",
    "    print(message.role + \": \" + message.content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"memgpt-paper.pdf\", \"rb\"),\n",
    "    purpose= \"assistants\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-2cIb3YGXlb8QqZVM69PipxLH', bytes=663708, created_at=1720044425, filename='memgpt-paper.pdf', object='file', purpose='assistants', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Machine Learning Researcher\",\n",
    "  instructions=\"'You are a machine learning researcher, answer questions on the research paper.'\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n",
      "VectorStoreFileBatch(id='vsfb_6f324825a5f44343b99a590d4aa9ce6a', created_at=1720047675, file_counts=FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1), object='vector_store.file_batch', status='completed', vector_store_id='vs_DtRHhJjJ0s0g1Ht0A9tYNzP8')\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.beta.vector_stores.create(name='memgpt-paper')\n",
    "file_paths = ['memgpt-paper.pdf']\n",
    "file_streams = [open(file_path, 'rb') for file_path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id,\n",
    "    files=file_streams,\n",
    ")\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "print(file_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ToolResourcesFileSearch(vector_store_ids=['vs_72u2sD9OtclutRQT2B9XTLG3'])\n"
     ]
    }
   ],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"./memgpt-paper.pdf\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    " \n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"How does memgpt allow LLMs to have unlimited context length?\",\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print()\n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = 'user',\n",
    "    content = 'How does memgpt allow LLMs to have unlimited context length?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m run \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mruns\u001b[38;5;241m.\u001b[39mcreate_and_poll(\n\u001b[1;32m      5\u001b[0m     thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid, assistant_id\u001b[38;5;241m=\u001b[39massistant\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mlist(thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid, run_id\u001b[38;5;241m=\u001b[39mrun\u001b[38;5;241m.\u001b[39mid))\n\u001b[0;32m---> 10\u001b[0m message_content \u001b[38;5;241m=\u001b[39m \u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m annotations \u001b[38;5;241m=\u001b[39m message_content\u001b[38;5;241m.\u001b[39mannotations\n\u001b[1;32m     12\u001b[0m citations \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
