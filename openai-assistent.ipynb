{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name='Math tutor',\n",
    "    instructions='You are a personal math tutor. Write and run code to answer math questions.',\n",
    "    tools=[{'type': 'code_interpreter'}],\n",
    "    model='gpt-3.5-turbo',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_hVCUAlrouysFazCHv53r0Ov0', created_at=1720050112, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create()\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = 'user',\n",
    "    content = 'Solve this problem: 3x + 11 = 14'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_URxGSf8m7KIdrCI4ASsIxhCJ', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Solve this problem: 3x + 11 = 14'), type='text')], created_at=1720050112, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_hVCUAlrouysFazCHv53r0Ov0')\n"
     ]
    }
   ],
   "source": [
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id = thread.id,\n",
    "    \n",
    "    assistant_id= assistant.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.retrieve(\n",
    "    thread_id = thread.id,\n",
    "    run_id = run.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[Message](data=[Message(id='msg_URxGSf8m7KIdrCI4ASsIxhCJ', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Solve this problem: 3x + 11 = 14'), type='text')], created_at=1720050112, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_hVCUAlrouysFazCHv53r0Ov0')], object='list', first_id='msg_URxGSf8m7KIdrCI4ASsIxhCJ', last_id='msg_URxGSf8m7KIdrCI4ASsIxhCJ', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id = thread.id\n",
    ")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Solve this problem: 3x + 11 = 14\n"
     ]
    }
   ],
   "source": [
    "for message in reversed(messages.data):\n",
    "    print(message.role + \": \" + message.content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Machine Learning Researcher\",\n",
    "  instructions=\"'You are a machine learning researcher, answer questions on the research paper.'\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n",
      "VectorStoreFileBatch(id='vsfb_d6b3d54639b944c895a3e407e716d54a', created_at=1720050116, file_counts=FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1), object='vector_store.file_batch', status='completed', vector_store_id='vs_gZiCiZUhQWUALQAOcYPE6dJZ')\n"
     ]
    }
   ],
   "source": [
    "#step 2\n",
    "\n",
    "vector_store = client.beta.vector_stores.create(name='memgpt-paper')\n",
    "file_paths = ['./memgpt-paper.pdf']\n",
    "file_streams = [open(file_path, 'rb') for file_path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id,\n",
    "    files=file_streams,\n",
    ")\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "print(file_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "\n",
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_SAROVR96NNpZcTFZ3ArtiktD'])\n"
     ]
    }
   ],
   "source": [
    "# step 4\n",
    "\n",
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"memgpt-paper.pdf\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    " \n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Summarize the research paper.\",\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(id='msg_I4u1m8na2dPShhiE9AkxDNap', assistant_id='asst_04ljt2Kh046vfq84Nj6UNXe5', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[FileCitationAnnotation(end_index=796, file_citation=FileCitation(file_id='file-h728z1UUQSHuGQyJ46FQDD2n'), start_index=784, text='【4:0†source】', type='file_citation'), FileCitationAnnotation(end_index=1555, file_citation=FileCitation(file_id='file-h728z1UUQSHuGQyJ46FQDD2n'), start_index=1543, text='【4:2†source】', type='file_citation')], value='The research paper titled \"MemGPT: Towards LLMs as Operating Systems\" introduces MemGPT, a system designed to address the limitations of Large Language Models (LLMs) due to their constrained context windows. The paper proposes virtual context management inspired by hierarchical memory systems in traditional operating systems to extend the context available to LLMs. MemGPT intelligently manages different storage tiers to effectively provide extended context within the LLM\\'s limited context window. The system is evaluated in document analysis, where it can process large documents exceeding traditional LLM context limits, and in multi-session chat applications, enabling conversational agents to remember, reflect, and evolve dynamically through long-term interactions with users【4:0†source】.\\n\\nThe work focuses on equipping agents with long-term memory of user inputs, contrasting with other approaches that augment LLMs with additional capabilities to act as agents in interactive environments. MemGPT\\'s design resembles traditional OS architectures, providing the illusion of larger context resources for LLMs. The paper concludes that MemGPT\\'s OS-inspired techniques like hierarchical memory management and interrupts can enhance the capabilities of LLMs even within fixed context length constraints, opening up avenues for future exploration including applying MemGPT to other domains with massive or unbounded contexts, integrating different memory tier technologies, and further improving control flow and memory management policies【4:2†source】.'), type='text')], created_at=1720050126, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_R1tqnEUFoRaKMNHK6S6gdDWA', status=None, thread_id='thread_Q77CCSpdnZNHNg3M8FBPuBVn')]\n",
      "The research paper titled \"MemGPT: Towards LLMs as Operating Systems\" introduces MemGPT, a system designed to address the limitations of Large Language Models (LLMs) due to their constrained context windows. The paper proposes virtual context management inspired by hierarchical memory systems in traditional operating systems to extend the context available to LLMs. MemGPT intelligently manages different storage tiers to effectively provide extended context within the LLM's limited context window. The system is evaluated in document analysis, where it can process large documents exceeding traditional LLM context limits, and in multi-session chat applications, enabling conversational agents to remember, reflect, and evolve dynamically through long-term interactions with users[0].\n",
      "\n",
      "The work focuses on equipping agents with long-term memory of user inputs, contrasting with other approaches that augment LLMs with additional capabilities to act as agents in interactive environments. MemGPT's design resembles traditional OS architectures, providing the illusion of larger context resources for LLMs. The paper concludes that MemGPT's OS-inspired techniques like hierarchical memory management and interrupts can enhance the capabilities of LLMs even within fixed context length constraints, opening up avenues for future exploration including applying MemGPT to other domains with massive or unbounded contexts, integrating different memory tier technologies, and further improving control flow and memory management policies[1].\n",
      "[0] memgpt-paper.pdf\n",
      "[1] memgpt-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "print(messages)\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = 'user',\n",
    "    content = 'How does memgpt allow LLMs to have unlimited context length?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemGPT allows Large Language Models (LLMs) to have an unlimited context length by implementing a virtual context management technique inspired by hierarchical memory systems in traditional operating systems. This technique provides the illusion of an extended virtual memory through paging between physical memory and disk. With MemGPT, different storage tiers are intelligently managed to effectively provide extended context within the LLM's limited context window. By allowing the LLM to manage what is placed in its own context via an 'LLM OS' like MemGPT, relevant historical data can be retrieved beyond the in-context data, and less relevant data can be evicted into external storage systems. This system combines a memory-hierarchy, OS functions, and event-based control flow to enable handling unbounded context using LLMs with finite context windows[0].\n",
      "[0] memgpt-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
